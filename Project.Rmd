---
title: "Predicting Whether a Kickstarter Project Will Succeed or Fail"
author: "Austin Martinez, Cameron Morefield, and Derek McFate"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This report will utilize various factors from a publicly provided Kickstarter data set to determine outcomes of a potential project The idea is to allow someone with a kickstarter idea to use their potential project's data to determine how successful it may be. 
<br/>

# Data Gathering and cleaning
The data set used contains 378,661 entries, and each entry has 15 variables. There were 3797
entries that had N/A values somewhere in the row, which we decided to remove from the data
set. The data set also included entries that had states other than 'Success' or 'Failure', so
those entries were either condensed into the two states or dropped completely. We also derived
a kicklen column from the launched and deadline dates so that we had another numeric field to
work with. 
Once the data was cleaned, it was split into training and test data.

```{r}
set.seed(123)
suppressWarnings(library(cluster))
library(e1071)
dat = read.csv("https://raw.githubusercontent.com/AMartinez2/CST383-Project/master/ks-projects-201801.csv")
dat = dat[complete.cases(dat),]
levels(dat$state)[which(levels(dat$state)=="canceled")] = "failed"
dat = dat[!(dat$state=="undefined" | dat$state=="suspended" | dat$state=="live"),]
dat$state = factor(dat$state)
# Date conversion:
ldates = as.Date(dat$launched, format = "%Y-%m-%d")
ddates = as.Date(dat$deadline, format = "%Y-%m-%d")
dat$kicklen = as.numeric(ddates - ldates)
# split our data into testing and training data
tr_rows = sample(nrow(dat), 0.8 * nrow(dat))
tr_dat = dat[tr_rows,]
te_dat = dat[-tr_rows,]
```

# Initial Exploration and Visualization

There may be some categories that are far more popular than others, which would then affect the success of a project. So we look at the differences in number of successes to number of failures per category. It would seem that there is a noticable drop off in differences with very few categories at the top. Category may be something that we want to use a as a predictor.

```{r}
pass = dat[dat$state=="successful",]
fail = dat[dat$state=="failed",]
diffs = abs(table(pass$main_category) - table(fail$main_category))
par(mar=c(3,9,1,1))
barplot(head(sort(diffs)), horiz="true", las=1, col="red", main="Top 6 Largest Pass/Fail Discrepancies in Catagories")
```

</br>

This was a method we used to see if any of our numeric data had value for our predictions. This graph plots the percentage of successes above and below a median predictor value, in this case, the Goal Amount (USD) and the length of the Kickstarter (# of days). If the percentage for a predictor is heavily weighted one way or another (difference is large), it may indicate that the predictor has some relationship with the success/fail result. A bar graph pairing that is nearer to 50/50 shows a weaker or no relationship.

This method of determining predictors is good when a predictor has a generally even distribution of numbers, but can be less accurate with more clumped data.

```{r}
#goal
medgoal = median(dat$goal)
n_success = nrow(dat[dat$state == "successful",])
n_size = nrow(dat)
under_med_goal_success = nrow(dat[dat$goal <= medgoal & dat$state == "successful",])/n_success
over_med_goal_success = nrow(dat[dat$goal > medgoal & dat$state == "successful",])/n_success
goalvec = c(under_med_goal_success,over_med_goal_success)

#length
medlen = median(dat$kicklen)
funder_med_len_success = nrow(dat[dat$kicklen <= medlen & dat$state == "successful",])/n_success
over_med_len_success = nrow(dat[dat$kicklen > medlen & dat$state == "successful",])/n_success
lenvecf = c(funder_med_len_success, over_med_len_success)
comb = c(goalvec, lenvecf)
barplot(comb, beside = TRUE, ylim = c(0, 1), col = c("green", "red"), main = "Analysis of available Numeric variables", names.arg = c("Goal1", "", "Length", ""),legend.text=c('Below or at Median','Above Median'), args.legend=list(text.col=c("green", "red"),bty='n'))
```

# First model (Naive Bayes)

For our first model, we decided to use Naive Bayes using only one predictor. This simple model yields roughly 66% percent accuracy and only guesses if a project will succeed or fail.

```{r}
fit = naiveBayes(state ~ category, data=tr_dat)
predicts = predict(fit, newdata=te_dat)
conf_mtx = table(predicts, te_dat$state)
mean(predicts == te_dat$state)
barplot(conf_mtx, beside = TRUE, col=c("red2", "green4"), main="Predicted vs Actual for Category", legend=c("Failure", "Success"))
```

The plot above shows that the model was more inclined to  predict that a project would fail instead of succeed. The rate at which the Naive Bayes model predicted failure is staggeringly high, which would mean that 'category' is a strong predictor for a project's success in the event of a failure. This model begins to fall apart when the outcome was actually a success however, as it predicts that a project will fail nearly twice as often as it will succeed, even when the true outcome was success.

# Second Model (Logistic Regression)

With our second model we can predict both the probability that a project will succeed as well as simply guess whether it will or not. First we build our logistic regression model using category, goal, and kicklen. With this, we can make predictions on the probability of success. 

```{r}
fit = suppressWarnings(glm(state ~ category + goal + kicklen, data=tr_dat, family=binomial))
y = predict(fit, newdata=te_dat, type="response")
plot(density(y), main="Density of Predictions", col="red")
```

From our probability predictions, we can choose a threshold to determine whether a project will succeed or fail. We will do so by iterating though the options from 0 to 1 and looking at the resulting accuracies. From there, we get our highest accuracy around 6. 

```{r}
acc = c()
actuals = te_dat$state
#conf_mtx = table(predicts, actuals)
for (i in 0:10) {
  predicts = ifelse(y > i/10, "successful", "failed")
  acc = c(acc, mean(predicts == actuals))
}
se = seq(0, 1, by=0.1)
plot(acc, type="l", col="red", main="Accuracy per Threshold", ylab="Accuracy", xlab="Threashold / 10")
```

```{r}
#precrec summary:
prec_recall_summary = function(predicts, actuals) {
  thresh = seq(0, 1, length.out=50)
    prec_rec = data.frame()
    actuals = factor(as.numeric(actuals))
    for (th in thresh) {
      predicts =factor(as.numeric(y>=th),levels=c("0","1"))
      prec_rec = rbind(prec_rec, as.vector(table(predicts, actuals)))
    }
    names(prec_rec) = c("TN", "FP", "FN", "TP")
    prec_rec$threshold = thresh
    prec_rec$precision = prec_rec$TP/(prec_rec$TP + prec_rec$FP)
    prec_rec$recall    = prec_rec$TP/(prec_rec$TP + prec_rec$FN)
    prec_rec$false_pos = prec_rec$FP/(prec_rec$FP + prec_rec$TN)
    return(prec_rec)
}
predicts1 = as.numeric(y >= 0.6)
lrsumm = prec_recall_summary(predicts1, actuals)
```

In order to analyze the effectiveness of our logistic model, we created graphs using precision, recall, and threshold values. Our first analylitical graph shows Precision plotted over Threshold.

```{r}
#precision ~ thresh
plot(lrsumm$precision ~ lrsumm$threshold, xlab = "Threshold", ylab = "Precision", pch = 16, main = "Precision over Threshold", col = "blue4")
lines(lrsumm$precision ~ lrsumm$threshold, col = "blue4")
```

The graph above shows that as the threshold value rises, the precision of the model slowly increases. The precision begins to get lower when the threshold is between 7.5 and 8, but once the threshold rises passed 8, the precision of the model becomes 1.

Our next graph plots Recall values over the Threshold.

```{r}
#recall ~ thresh
plot(lrsumm$recall ~ lrsumm$threshold, xlab = "Threshold", ylab = "Recall", pch = 16, main = "Recall over Threshold", col = "blue4")
lines(lrsumm$recall ~ lrsumm$threshold, col = "blue4")
```

As expected, this graph scales inversely with the Precision/Recall graph. As the threshold increases, the recall of the model goes down until the threshold reaches 8. At that point, the recall for the model is zero.

We also constructed an ROC Curve to act as a diagnostic representation of our logistic model.

```{r}
#ROC Curve
normtp = (lrsumm$TP - min(lrsumm$TP))/ (max(lrsumm$TP) - min(lrsumm$TP))
normfp = (lrsumm$FP - min(lrsumm$FP))/ (max(lrsumm$FP) - min(lrsumm$FP))

plot(normtp ~ normfp, xlab = "FPR", ylab = "TPR", main = "ROC Curve")
lines(normtp ~ normfp)
abline(0, 1, col = "red2", lty = 2)
```

The model is not very effective at making predictions, as the curve stayed relatively close to the central y = x base line.
The rate of increase seems to taper off around 0.2. The ROC curve also did not have a well defined elbow as we had initally expected. If our predictions were more robust, we would expect a more pronounced logarithmic line.

# Conclusion

### Evaluation of Models

Both of the models created here peaked at a ~65% prediction success rate. Naive Bayes had a rate that was surprising similar to our logistic regression model, even though it only used the "category" attribute as a predictor. Our Logistic Regression model utilized "category", "goal", and "length" of the kickstarter as predictors. in our exploration, these were all the most interesting predictors. Although Naive Bayes and Logistic regression use different algorithms to model, the fact that the accuracy for both was similar may mean that Goal and Length have less of an impact than we initially thought.

### Overall thoughts

Our modeling showed that there is some potential patterning between different Kickstarters. It may be possible to get a weak sense of whether or not a product will succeed or fail, but otherwise, more sutble factors probably have a much greater impact on the success or fail rate. Usually a Kickstarter that has an open development process, clearly defined goals and ideas, as well as an experienced team for the Kickstarter all play enormously larger roles in if a Kickstarter succeeds. These attributes are enormously hard to quantify with standard data sets.     