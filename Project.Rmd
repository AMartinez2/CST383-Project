---
title: "Predicting Whether a Kickstarter Project Will Succeed Or Fail"
author: "Austin Martinez, Cameron Morefield, and Derek McFate"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
This report will utilize various factors from a publicly provided Kickstarter data set to determine outcomes of a potential project The idea is to allow someone with a kickstarter idea to use their potential project's data to determine how successful it may be. 
<br/>

# Data Gathering and cleaning
The data set used contains 378,661 entries, and each entry has 15 variables. There were 3797
entries that had N/A values somewhere in the row, which we decided to remove from the data
set. The data set also included entries that had states other than 'Success' or 'Failure', so
those entries were either condensed into the two states or dropped completely. We also derived
a kicklen column from the launched and deadline dates so that we had another numeric field to
work with. 
Once the data was cleaned, it was split into training and test data.

```{r}
set.seed(123)
library(cluster)
library(e1071)
dat = read.csv("https://raw.githubusercontent.com/AMartinez2/CST383-Project/master/ks-projects-201801.csv")
dat = dat[complete.cases(dat),]
levels(dat$state)[which(levels(dat$state)=="canceled")] = "failed"
dat = dat[!(dat$state=="undefined" | dat$state=="suspended" | dat$state=="live"),]
dat$state = factor(dat$state)
# Date conversion:
ldates = as.Date(dat$launched, format = "%Y-%m-%d")
ddates = as.Date(dat$deadline, format = "%Y-%m-%d")
dat$kicklen = as.numeric(ddates - ldates)
# split our data into testing and training data
tr_rows = sample(nrow(dat), 0.8 * nrow(dat))
tr_dat = dat[tr_rows,]
te_dat = dat[-tr_rows,]
```

# Initial Exploration and visualization

```{r}
#goal
medgoal = median(dat$goal)
n_success = nrow(dat[dat$state == "successful",])
n_size = nrow(dat)
under_med_goal_success = nrow(dat[dat$goal <= medgoal & dat$state == "successful",])/n_success
over_med_goal_success = nrow(dat[dat$goal > medgoal & dat$state == "successful",])/n_success
goalvec = c(under_med_goal_success,over_med_goal_success)
a
#length
medlen = median(dat$kicklen)
funder_med_len_success = nrow(dat[dat$kicklen <= medlen & dat$state == "successful",])/n_success
tunder_med_len_success = nrow(dat[dat$kicklen < medlen & dat$state == "successful",])/n_success
over_med_len_success = nrow(dat[dat$kicklen > medlen & dat$state == "successful",])/n_success
true_med_len_success = nrow(dat[dat$kicklen == medlen & dat$state == "successful",])/n_success
tunder_med_len_success
over_med_len_success
true_med_len_success
lenvecf = c(funder_med_len_success, over_med_len_success)
lenvect = c(tunder_med_len_success, true_med_len_success, over_med_len_success)
comb = c(goalvec, lenvecf)
par(mfrow = c(2,1))
barplot(comb, beside = TRUE, ylim = c(0, 1), col = c("green", "red"), main = "Analysis of available Numeric variables", names.arg = c("Goal1", "", "Length", ""),legend.text=c('Below or at Median','Above Median'), args.legend=list(text.col=c("green", "red"),bty='n'))
```
This was a method we used to see if any of our numeric data had value for our predictions. This graph plots the percentage of successes above and below a median predictor value, in this case, the Goal Amount (USD) and the length of the Kickstarter (# of days). If the percentage for a predictor is heavily weighted one way or another (difference is large), it may indicate that the predictor has some relationship with the success/fail result. A bar graph pairing that is nearer to 50/50 shows a weaker or no relationship.

This method of determining predictors is good when a predictor has a generally even distribution of numbers, but can be less accurate with more clumped data. Due to this, goal was a much better predictor than length.

# First model (Naive Bayes)

For our first model, we decided to use Naive Bayes using only one predictor. This simple model yields roughly 66% percent accuracy and only guesses if a project will succeed or fail.

```{r}
fit = naiveBayes(state ~ category, data=tr_dat)
predicts = predict(fit, newdata=te_dat)
conf_mtx = table(predicts, te_dat$state)
conf_mtx
mean(predicts == te_dat$state)
barplot(conf_mtx, beside = TRUE, col=c("red2", "green4"), main="Predicted vs Actual for Category", legend=c("Failure", "Success"))
```

The plot above shows that the model was more inclined to  predict that a project would fail instead of succeed. The rate at which the Naive Bayes model predicted failure is staggeringly high, which would mean that 'category' is a strong predictor for a project's success in the event of a failure. This model begins to fall apart when the outcome was actually a success however, as it predicts that a project will fail nearly twice as often as it will succeed, even when the true outcome was success.

# Second Model (Logistic Regression)

With our second model we can predict both the probability that a project will succeed as well as simply guess whether it will or not. First we build our logistic regression model using [insert final options here]. With this, we can make predictions on the probability of success. 

```{r}
fit = glm(state ~ category + goal + kicklen, data=tr_dat, family=binomial)
y = predict(fit, newdata=te_dat, type="response")
plot(density(y), main="Density of Predictions")
```

From our probability predictions, we can choose a threshold to determine whether a project will succeed or fail. We will do so by iterating though the options from 0 to 1 and looking at the resulting accuracies. From there, we get our highest accuracy around 6. 

```{r}
acc = c()
actuals = te_dat$state
#conf_mtx = table(predicts, actuals)
for (i in 0:10) {
  predicts = ifelse(y > i/10, "successful", "failed")
  acc = c(acc, mean(predicts == actuals))
}
plot(acc, type="l", col="red", main="Accuracy per Threshold")
```

```{r}
#precrec summary:
prec_recall_summary = function(predicts, actuals) {
  thresh = seq(0, 1, length.out=50)
    prec_rec = data.frame()
    actuals = factor(as.numeric(actuals))
    for (th in thresh) {
      predicts =factor(as.numeric(y>=th),levels=c("0","1"))
      prec_rec = rbind(prec_rec, as.vector(table(predicts, actuals)))
    }
    names(prec_rec) = c("TN", "FP", "FN", "TP")
    prec_rec$threshold = thresh
    prec_rec$precision = prec_rec$TP/(prec_rec$TP + prec_rec$FP)
    prec_rec$recall    = prec_rec$TP/(prec_rec$TP + prec_rec$FN)
    prec_rec$false_pos = prec_rec$FP/(prec_rec$FP + prec_rec$TN)
    return(prec_rec)
}

predicts1 = as.numeric(y >= 0.6)
lrsumm = prec_recall_summary(predicts1, actuals)
normtp = (lrsumm$TP - min(lrsumm$TP))/ (max(lrsumm$TP) - min(lrsumm$TP))
normfp = (lrsumm$FP - min(lrsumm$FP))/ (max(lrsumm$FP) - min(lrsumm$FP))

plot(normtp ~ normfp, xlab = "FPR", ylab = "TPR", main = "ROC Curve")
lines(normtp ~ normfp)

```

# Conclusion

